{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:green;'><center>Various Neural Network Architect</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the\n",
    "activation function?</h3>\n",
    "\n",
    "Feedforward Neural Network (FNN): An FNN is a type of artificial neural network where connections between the nodes do not form any cycles. It consists of:\n",
    "\n",
    "- Input Layer: The first layer, where data is fed into the network.\n",
    "- Hidden Layers: Layers where computation occurs through a series of neurons (nodes). Each hidden layer passes information to the next layer in a forward-only manner.\n",
    "- Output Layer: The final layer that produces the prediction or output.\n",
    "- Purpose of the Activation Function: Activation functions introduce non-linearities into the network, enabling it to learn and approximate complex functions. Without activation functions, the network would be limited to linear transformations, reducing its capability to solve complex, real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2 Explain the role of convolutional layers in a CNN. Why are pooling layers commonly used, and what do\n",
    "they achieve?</h3>\n",
    "\n",
    "- Convolutional Layers:\n",
    "    - In a Convolutional Neural Network (CNN), convolutional layers are used to automatically and adaptively learn spatial hierarchies of features from input images. They apply a set of filters (kernels) that slide over the image to detect patterns such as edges, textures, and more complex structures as depth increases. This enables CNNs to capture spatial and temporal dependencies in images.\n",
    "\n",
    "- Pooling Layers:\n",
    "    - Pooling layers are commonly used to reduce the spatial dimensions (width and height) of the feature maps, which helps in decreasing computational complexity and prevents overfitting. The most common pooling method is max pooling, which takes the maximum value from each patch of the feature map. Pooling layers improve the network’s robustness to small translations and distortions in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural\n",
    "networks? How does an RNN handle sequential data?</h3>\n",
    "\n",
    "- Key Characteristic: \n",
    "    - RNNs are distinguished by their ability to maintain a \"memory\" of previous inputs, enabling them to capture sequential dependencies. Unlike feedforward networks, RNNs have connections that loop back, allowing information from previous steps in a sequence to influence the current processing step.\n",
    "\n",
    "- Handling Sequential Data: \n",
    "    - RNNs process sequences by maintaining a hidden state that is updated at each time step. As each input in a sequence is processed, the network updates its hidden state, effectively encoding past information. This makes RNNs well-suited for tasks involving sequential data, such as language modeling, time series analysis, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the\n",
    "vanishing gradient problem?</h3>\n",
    "\n",
    "- Components of an LSTM: \n",
    "  LSTMs are a type of RNN designed to retain long-term dependencies. Each LSTM cell has three primary components:\n",
    "\n",
    "    - Input Gate: Determines how much of the current input should be used to update the cell state.\n",
    "    - Forget Gate: Controls which part of the previous cell state should be forgotten.\n",
    "    - Output Gate: Decides how much of the current cell state should be output to the next layer.\n",
    "\n",
    "- Addressing the Vanishing Gradient Problem: \n",
    "    - LSTMs address this issue by using a mechanism called cell state, which runs through the network with minor linear transformations, allowing gradients to flow more easily through time. The gates enable the network to retain or discard information, which helps to prevent gradients from vanishing during training, making it possible to learn dependencies over longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is\n",
    "the training objective for each?</h3>\n",
    "\n",
    "- Generator: The generator in a GAN creates fake data (e.g., images) that resemble real data. Its objective is to learn the data distribution and generate data samples that are indistinguishable from real ones. The generator’s training goal is to maximize the discriminator's error rate by producing more realistic outputs.\n",
    "\n",
    "- Discriminator: The discriminator’s role is to distinguish between real data and fake data generated by the generator. It receives both real data samples and the generator's fake samples, learning to classify each correctly. The discriminator's objective is to minimize its error by correctly identifying real and fake samples.\n",
    "\n",
    "- Training Objectives:\n",
    "\n",
    "    - Generator: Maximize the probability that the discriminator classifies its generated samples as real.\n",
    "    - Discriminator: Minimize the probability of mistakenly identifying fake data as real and vice versa."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
