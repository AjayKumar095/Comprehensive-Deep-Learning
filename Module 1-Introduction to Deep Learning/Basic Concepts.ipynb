{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:green;'><center>Basic Concepts Neural Networks</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural Networks: Introduction and History</h3>\n",
    "\n",
    "1. Introduction\n",
    "Neural Networks (NNs) are computational models inspired by the structure and functioning of biological neurons. They consist of interconnected nodes, known as neurons or units, organized in layers: the input layer, hidden layers, and output layer.<br>\n",
    "Each neuron in the network processes inputs, applies a weight to each input, sums them up, passes this sum through an activation function, and then produces an output. Mathematically, the process for a single neuron can be expressed as:<br><br>\n",
    "z = (limit i = 1 up to n), ∑ w<sub>i</sub> . x<sub>i</sub> + b\n",
    "<br><br>\n",
    "where:\n",
    "\n",
    "- x<sub>i</sub> represents the input features,\n",
    "- w<sub>i</sub> are the corresponding weights,\n",
    "- b is the bias term, and\n",
    "- z is the weighted sum.<br><br>\n",
    "This weighted sum is passed through an activation function σ(z), which introduces non-linearity into the network, allowing it to model complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. History\n",
    "- 1943: Warren McCulloch and Walter Pitts proposed the first model of a neural network, the McCulloch-Pitts Neuron.\n",
    "- 1958: Frank Rosenblatt introduced the Perceptron, an early type of neural network that could classify data.\n",
    "- 1969: Marvin Minsky and Seymour Papert's book highlighted the limitations of perceptrons, leading to a decline in interest.\n",
    "- 1980s: The concept of backpropagation, introduced by Geoffrey Hinton and others, reignited interest in neural networks by making it feasible to train deep networks.\n",
    "- 2000s-Present: The advent of powerful GPUs, large datasets, and advances in algorithms (e.g., ReLU activation and Dropout) led to the era of Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The Importance of Data in Deep Learning\n",
    "Data is critical in deep learning for several reasons:\n",
    "\n",
    "- Training Accuracy: Large datasets with diverse and high-quality samples enable neural networks to generalize better and make more accurate predictions.\n",
    "\n",
    "- Overfitting vs. Generalization: A small or non-representative dataset can cause the model to overfit (memorize the training data but perform poorly on unseen data).\n",
    "\n",
    "- Representation Learning: Neural networks, especially deep networks, automatically learn to represent data in useful ways (e.g., through layers that progressively extract more abstract features).\n",
    "\n",
    "For example, in image recognition, raw pixel data is transformed into hierarchical feature representations. Convolutional layers detect edges, textures, and finally, whole objects.\n",
    "\n",
    "Practical Consideration\n",
    "\n",
    "A well-prepared dataset undergoes various preprocessing steps:\n",
    "\n",
    "- Normalization/Standardization: To ensure that features are on similar scales, often required for gradient-based optimization algorithms.\n",
    "- Augmentation: Increasing the diversity of the dataset by applying transformations such as rotations, flips, or color changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
