{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:green;'><center>Forward and Backward\n",
    "Propagation</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q1. Explain the concept of forward propagation in a neural network'</h3>\n",
    "Forward propagation is the process by which input data is passed through the layers of a neural network to generate an output. In this process, the input is multiplied by weights, and biases are added as the data moves from layer to layer. An activation function is applied to introduce non-linearity, allowing the network to learn complex patterns.\n",
    "\n",
    "In forward propagation:\n",
    "\n",
    "- Input data is multiplied by weights at each neuron.\n",
    "- Biases are added to these weighted sums.\n",
    "- Activation functions are applied to determine the neuronâ€™s output.\n",
    "- The result is passed to the next layer until reaching the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q2. What is the purpose of the activation function in forward propagation</h3>\n",
    "The activation function introduces non-linearity into the network, allowing it to learn complex mappings between inputs and outputs. Without activation functions, the neural network would behave like a linear regression model, regardless of its depth, since combining linear transformations without non-linearity is still a linear transformation.\n",
    "\n",
    "Common Activation Functions:\n",
    "\n",
    "- Sigmoid: Maps inputs to a range between 0 and 1, useful in binary classification.\n",
    "- ReLU (Rectified Linear Unit): Outputs zero for negative values and the input itself for positive values, commonly used due to efficient computation and improved gradient flow.\n",
    "- Tanh: Maps inputs to a range between -1 and 1, often used for models where negative values need to be expressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q3.Describe the steps involved in the backward propagation (backpropagation) algorithm.</h3>\n",
    "Backpropagation is the process by which a neural network adjusts its weights and biases based on the error (or loss) in its predictions. It minimizes the error by using the gradient descent optimization method.\n",
    "\n",
    "Steps in backpropagation:\n",
    "\n",
    "- Calculate Loss: Compute the error between predicted and actual output.\n",
    "- Compute Gradients: Using the chain rule, calculate the gradients of the loss with respect to each weight and bias.\n",
    "- Update Weights and Biases: Using these gradients, update the weights and biases to reduce the loss.\n",
    "- Repeat: Iterate over multiple epochs, repeating forward and backward propagation, until the model converges (i.e., loss becomes minimal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q4.What is the purpose of the chain rule in backpropagation.</h3>\n",
    "The chain rule is very important in backpropagation as it allows for the calculation of gradients with respect to all weights and biases in the network. Since the network has multiple layers, we need to use the chain rule to compute how the final loss depends on each parameter by considering how each parameter affects the next layer in a sequence.\n",
    "\n",
    "In backpropagation:\n",
    "\n",
    "- The chain rule helps calculate the derivative of the loss function with respect to each weight and bias.\n",
    "- These derivatives (gradients) indicate how much each parameter should change to reduce the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Q5.Implement the forward propagation process for a simple neural network with one hidden layer using\n",
    "NumPy.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of forward propagation: [[1.01643798]]\n"
     ]
    }
   ],
   "source": [
    "## Implement the forward propagation process\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    val= 1/1+np.exp(-x)\n",
    "    return val\n",
    "\n",
    "# Define the forward propagation function\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Step 1: Calculate the input to the hidden layer (Z1)\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    \n",
    "    # Step 2: Apply activation function on the hidden layer (A1)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    # Step 3: Calculate the input to the output layer (Z2)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    \n",
    "    # Step 4: Apply activation function on the output layer (A2)\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    return A2\n",
    "\n",
    "# Example setup\n",
    "np.random.seed(0)\n",
    "X = np.array([[0.5, 0.2]])  # Example input data (1 sample, 2 features)\n",
    "W1 = np.random.randn(2, 3)  # Weights for layer 1 (2 inputs, 3 neurons)\n",
    "b1 = np.random.randn(1, 3)  # Biases for layer 1 (3 neurons)\n",
    "W2 = np.random.randn(3, 1)  # Weights for layer 2 (3 inputs from hidden layer, 1 output)\n",
    "b2 = np.random.randn(1, 1)  # Biases for layer 2 (1 output neuron)\n",
    "\n",
    "# Perform forward propagation\n",
    "output = forward_propagation(X, W1, b1, W2, b2)\n",
    "print(\"Output of forward propagation:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
